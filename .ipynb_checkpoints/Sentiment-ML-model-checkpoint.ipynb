{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 40 color=darkgreen>Sentiment Analysis</font><br>\n",
    "Sentiment analysis is the process of understanding the opinion of an author about a subject. This is comprised of 3 elements:\n",
    "1. The opinion (*Positive - Neutral - Negative*) and/or emotion (*Joy - Surprise - Anger - Disgust*) \n",
    "2. The subject being talked about\n",
    "3. Opinion holder<br>\n",
    "\n",
    "Sentiment analysis is used to give insight how people are talking about a subject.<BR> \n",
    "Some common areas of application are:\n",
    "- Social media monitoring\n",
    "- Brand monitoring\n",
    "- Customer service\n",
    "- Product analysis\n",
    "- Market research and analysis<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5 color=darkgreen>Movie Review Model</font><br>\n",
    "Using Logistic regression to predict the probability of sentiment is positive or negative given a movie review.<br>\n",
    "This model will be using a cleaned version of the data found on Kaggle:<br> https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews#*<br><br>\n",
    "Please see other notebook for cleaning the data:<BR>\n",
    "https://github.com/michael-william/Sentiment-analysis/blob/master/Sentiment-analysis.ipynb<br><br>\n",
    "**Steps for building the model:**<br>\n",
    "1. Importing libraries and reading data\n",
    "2. Organizing the data \n",
    "3. Tokenizing the data\n",
    "4. Remove noise from data\n",
    "5. Normalizing the data\n",
    "6. Checking the word density\n",
    "7. Preparing the model\n",
    "8. Building the model\n",
    "9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal>Import data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelcondon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michaelcondon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/michaelcondon/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from wordcloud import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from langdetect import detect_langs\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "import re, string\n",
    "\n",
    "\n",
    "#ML libraries\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "# autoreload extension\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from Git\n",
    "source = 'https://github.com/michael-william/Sentiment-analysis/raw/master/IMDB_Dataset.csv'\n",
    "df = pd.read_csv(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal> Organizing the data</font>\n",
    "Need to split the data we have into 'train' and 'test' sets so that we can train the machine on only some of the data, and then test the machine on known data to check for accuracy.<br>\n",
    "The final output of this phase should be 2 dataframes(train and test) created from the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = df[df.sentiment=='positive']\n",
    "neg_df = df[df.sentiment=='negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_strings = list(pos_df.review.astype('str'))\n",
    "neg_strings = list(neg_df.review.astype('str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal> Tokenizing the data</font>\n",
    "We need to process the language of the reviews into a format that can be understood by the machine. Tokenization will split the string version of the reviews into smaller parts called tokens. Tokens can then consist of words, emoticons, hashtags, etc.<br>\n",
    "The final output of this phase should be 2 objects(pos_tokens and neg_tokens) that are lists of lists of reviews where each element of a review is itemized.<br>\n",
    "Example of an tokenized review should look like this:<br>\n",
    "['A', 'wonderful', 'little',' production', '.', '<', 'br', '/', '>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing positive and negative reviews\n",
    "pos_tokens = [word_tokenize(x) for x in pos_strings]\n",
    "neg_tokens = [word_tokenize(x) for x in neg_strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal> Remove Noise</font>\n",
    "Noise is any part of the text that does not add meaning of information. Stopwords are a type of noise and consist of words like 'a', 'the', 'and.' Other noise can be hyper links, symbols, and other special characters.<br>\n",
    "The final output of this phase will be a 'de-noised' version of our token objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to remove noise from our tokenized lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing a frozen set of stop words\n",
    "stop_words = ENGLISH_STOP_WORDS.union([\"'s\",'br','film','movie',\"''\",'``',\"n't\",\n",
    "                                      '1/2',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for removing noise\n",
    "\n",
    "def remove_noise(tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*/\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"\",token) #replacing hyperlinks with spaces\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\",token) # replacing scpecial characters with spaces\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words and token.isalpha()==True:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our noise removal function to create clean token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of clean pos_tokens\n",
    "clean_pos_tokens = [remove_noise(x,stop_words) for x in pos_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of clean neg_tokens\n",
    "clean_neg_tokens = [remove_noise(x,stop_words) for x in neg_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal> Normalizing the data</font>\n",
    "Words have different forms - for instance, 'ran', 'run', and 'running' are various forms of the same verb 'run.' Normalization is the process of converting a word to its canoical form. It helps with grouping words with the same meaning but different forms.<br>\n",
    "We will be using lemmatization which normalizes a word with the context of vocabulary and morphilogical analysis of words in text.<br>A few steps that need to be completed:<br>\n",
    "1. We will use 'wordnet' from nltk as a lexical database for the English language to help determine the base word.(downloaded at the begining of this notebook) \n",
    "2. We will use 'averaged_perceptron_tragger' as a resource to determine the context of a word in a sentence\n",
    "3. We will use pos_tag to provide the relative position of the word in the sentence<br> Stemming is also a method to normalize, which is faster, but not as accurate as lemmatization.\n",
    "\n",
    "The output of this phase is to normalize every tokenized review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to lemmatize our token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes a list of tokens and applies lemmatization\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer() # initiating WordLemmatizer\n",
    "    lemmatized_sentence = [] #creating empty list to hold words after they've been analyzed\n",
    "    for word, tag in pos_tag(tokens): # iterating through tokens and their relative position\n",
    "        if tag.startswith('NN'): # NN=Noun\n",
    "            pos='n'\n",
    "        elif tag.startswith('VB'): # VB=Verb\n",
    "            pos='v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word,pos)) # appending the word and position\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our lemmatize function to create new cleaned token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lemmatized = [lemmatize_sentence(x) for x in clean_pos_tokens]\n",
    "neg_lemmatized = [lemmatize_sentence(x) for x in clean_neg_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal> Determine word density</font>\n",
    "We want to create a bank of words associated with both our positive and negative, cleaned, tokenized lists.\n",
    "The end output of this phase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a function to get unqie words from a cleaned token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens):\n",
    "    for tokens in cleaned_tokens:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for top common words in positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_pos = FreqDist(get_all_words(pos_lemmatized)).most_common(10)\n",
    "freq_dist_neg = FreqDist(get_all_words(neg_lemmatized)).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "cell_style": "split",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 19746),\n",
       " ('like', 19146),\n",
       " ('good', 19095),\n",
       " ('time', 15397),\n",
       " ('great', 14186),\n",
       " ('just', 14043),\n",
       " ('character', 13712),\n",
       " ('story', 13711),\n",
       " ('make', 13036),\n",
       " ('watch', 12059)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 24821),\n",
       " ('like', 23411),\n",
       " ('just', 21001),\n",
       " ('bad', 20816),\n",
       " ('good', 20800),\n",
       " ('make', 15215),\n",
       " ('time', 14366),\n",
       " ('watch', 14057),\n",
       " ('character', 14046),\n",
       " ('really', 12293)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal> Prepare the model</font>\n",
    "First, we will prepare the data to be fed into the model. We will use the Naive Bayes classifier in NLTK to perform the modeling exercise. The model requires not just a list of words in a review, but a Python dictionary with words as keys and True as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator function to change the format of the cleaned data to dictionary format\n",
    "def get_tokens_for_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying generator to pos_lemmatized list\n",
    "positive_dataset = [(token_dict, \"Positive\")\n",
    "                     for token_dict in get_tokens_for_model(pos_lemmatized)]\n",
    "\n",
    "# applying generator to neg_lemmatized list\n",
    "negative_dataset = [(token_dict, \"Negative\")\n",
    "                     for token_dict in get_tokens_for_model(neg_lemmatized)]\n",
    "\n",
    "# combining pos and neg to get a complete dataset\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "# shuffling order of full dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# splitting  the full dataset in to train and test sets\n",
    "total_observations = len(dataset)\n",
    "cutoff = int(np.ceil((total_observations * .5)))\n",
    "train_data = dataset[:cutoff]\n",
    "test_data = dataset[cutoff:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal> Build and test the model</font>\n",
    "We will be using the NaiveBayesClassifier to build the model. The output gives us the probability of a positive sentiment given a writter review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.85728\n",
      "Most Informative Features\n",
      "                     uwe = True           Negati : Positi =     30.1 : 1.0\n",
      "             abomination = True           Negati : Positi =     29.4 : 1.0\n",
      "                    boll = True           Negati : Positi =     28.1 : 1.0\n",
      "                bearable = True           Negati : Positi =     24.1 : 1.0\n",
      "             unwatchable = True           Negati : Positi =     22.4 : 1.0\n",
      "            collaborator = True           Positi : Negati =     20.5 : 1.0\n",
      "               geraldine = True           Positi : Negati =     19.2 : 1.0\n",
      "                   felix = True           Positi : Negati =     16.7 : 1.0\n",
      "                  farley = True           Positi : Negati =     16.5 : 1.0\n",
      "               camcorder = True           Negati : Positi =     16.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initiating the classieifer and training it on the train data\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# checking the accuracy of the model by applying it to the test data\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "# getting the top 10 words impacting the model\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=teal> Conclusion</font>\n",
    "This model was able to correctly guess the sentiment(positive or negative) with an accuracy of 85.7% by analyzing a written review.  This is an improvement from the basic 'polarity' calculation from the TextBlob library. The polarity calculation was only accurate to 71%, 14 percentage points lower than our new model. However, this new model is computationaly heavy and takes a long time to train. Deployment on a large scale may be difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All code needed to build and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, WordNetLemmatizer, classify, NaiveBayesClassifier\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "#ML libraries\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Reading data from Git\n",
    "source = 'https://github.com/michael-william/Sentiment-analysis/raw/master/IMDB_Dataset.csv'\n",
    "df = pd.read_csv(source)\n",
    "\n",
    "# Splitting data into positive and negative dataframes\n",
    "pos_df = df[df.sentiment=='positive']\n",
    "neg_df = df[df.sentiment=='negative']\n",
    "\n",
    "# Converting review columns of dataframes to a list of strings\n",
    "pos_strings = list(pos_df.review.astype('str'))\n",
    "neg_strings = list(neg_df.review.astype('str'))\n",
    "\n",
    "# Tokenizing positive and negative lists\n",
    "pos_tokens = [word_tokenize(x) for x in pos_strings]\n",
    "neg_tokens = [word_tokenize(x) for x in neg_strings]\n",
    "\n",
    "# Defing a frozen set of stop words with additional elements specifically for this dataset\n",
    "stop_words = ENGLISH_STOP_WORDS.union([\"'s\",'br','film','movie',\"''\",'``',\"n't\"])\n",
    "\n",
    "# Function for removing noise\n",
    "def remove_noise(tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*/\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+',\"\",token) #replacing hyperlinks with spaces\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\",token) # replacing scpecial characters with spaces\n",
    "        \n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words and token.isalpha()==True:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Creating list of clean pos_tokens and neg_tokens\n",
    "clean_pos_tokens = [remove_noise(x,stop_words) for x in pos_tokens]\n",
    "clean_neg_tokens = [remove_noise(x,stop_words) for x in neg_tokens]\n",
    "\n",
    "# A function that takes a list of tokens and applies lemmatization\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer() # initiating WordLemmatizer\n",
    "    lemmatized_sentence = [] #creating empty list to hold words after they've been analyzed\n",
    "    for word, tag in pos_tag(tokens): # iterating through tokens and their relative position\n",
    "        if tag.startswith('NN'): # NN=Noun\n",
    "            pos='n'\n",
    "        elif tag.startswith('VB'): # VB=Verb\n",
    "            pos='v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word,pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Lemmatizing cleaned tokens\n",
    "pos_lemmatized = [lemmatize_sentence(x) for x in clean_pos_tokens]\n",
    "neg_lemmatized = [lemmatize_sentence(x) for x in clean_neg_tokens]\n",
    "\n",
    "# Generator function to change the format of the cleaned data to dictionary format\n",
    "def get_tokens_for_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "\n",
    "# applying generator to pos_lemmatized list\n",
    "positive_dataset = [(token_dict, \"Positive\")\n",
    "                     for token_dict in get_tokens_for_model(pos_lemmatized)]\n",
    "\n",
    "# applying generator to neg_lemmatized list\n",
    "negative_dataset = [(token_dict, \"Negative\")\n",
    "                     for token_dict in get_tokens_for_model(neg_lemmatized)]\n",
    "\n",
    "# combining pos and neg to get a complete dataset\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "# shuffling order of full dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# splitting  the full dataset into equal size train and test sets\n",
    "total_observations = len(dataset)\n",
    "cutoff = int(np.ceil((total_observations * .5)))\n",
    "train_data = dataset[:cutoff]\n",
    "test_data = dataset[cutoff:]\n",
    "\n",
    "# Initiating the classieifer and training it on the train data\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "# checking the accuracy of the model by applying it to the test data\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "# getting the top 10 words impacting the model\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Special thanks to Shaumik Daityari and his walk-through on sentiment analysis which was the base used for this notebook***\n",
    "https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
